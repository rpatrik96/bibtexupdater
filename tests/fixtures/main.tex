\documentclass{article}
\usepackage{natbib}

\begin{document}

\section{Introduction}
% Test standard \cite command with foundational deep learning paper
Deep learning has revolutionized machine learning \cite{goodfellow2016deeplearning}.

% Test \citep (parenthetical citation) with optimization paper
Modern neural networks are typically trained using adaptive optimizers \citep{kingma2015adam}.

% Test \citet (textual citation) with the Transformer paper
\citet{vaswani2017attention} introduced the Transformer architecture that revolutionized NLP.

% Test multiple citations in one command
Residual connections and batch normalization are key components \cite{he2016resnet, ioffe2015batchnorm}.

% This is a comment - citations here should be IGNORED: \cite{should_be_ignored}
% \citep{also_ignored}

% Test citation with optional argument - referencing the Deep Learning book
According to \cite[Chapter 9]{goodfellow2016deeplearning}, convolutional networks are essential for vision.

% Test citation with pre and post notes
As noted by \citep[see][Section 3]{vaswani2017attention}, self-attention enables parallel computation.

\section{Generative Models}
% Test GANs and diffusion models
Generative adversarial networks \citep{goodfellow2014gan} pioneered deep generative modeling,
while diffusion models \citep{ho2020ddpm} have recently achieved state-of-the-art results.

\section{Language Models}
% Test LLM citations (arXiv preprints)
Large language models like GPT-3 \citep{brown2020gpt3} demonstrate remarkable few-shot learning.
Open-source alternatives like LLaMA \citep{touvron2023llama} have democratized access to LLMs.

\section{Regularization}
% Test dropout citation
Dropout remains a popular regularization technique \citep{srivastava2014dropout}.

\end{document}
